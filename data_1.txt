Abstract— Game playing is the best area to test Artificial intelligence since it can used to generate responsive and adaptive behavior, similar to human-like intelligence. Arcade games are available in abundance on online. They are easy to play games but hard to master for humans. Whereas an AI can be trained to master such games with the AI technique “Reinforcement Learning”. Reinforcement trained bot can surpass the human who are expert in those games, but they also face certain challenges while learning to master certain arcade games. This paper illustrates the most challenging problem in playing arcade games using reinforcement learning and how it can be overcome using Partial Supervised Reinforcement learning where a bot trained with the help of existing human game play data.
 
Keywords—Artificial Intelligence; Neural network; Arcade Games; Reinforcement learning; Inverse Reinforcement learning.
I.	INTRODUCTION
Artificial neural network was able to solve some problems which contain uncertainty inside, which usually only human could solve. The objective of this research was to find the effectiveness of such method of artificial neural network, reinforced learning. Here in reinforced learning method, for various online arcade games, a game bot is developed to use deep learning neural networks that would have no game-specific knowledge.  This technique observes the game’s previous state and reward (such as the pixels seen on the screen or the game score). It then comes up with an action to perform on the environment. This game bot beats the game based on the pixels they saw on screen and their knowledge of the game controls.
The mathematical framework for defining a solution in reinforcement learning scenario is called Markov Decision Process.
This can be designed as:
Set of states, S
Set of actions, A
Reward function, R
Policy, π
Value, V
We have to take an action (A) to transition from our start state to our end state (S). In return we get rewards (R) for each action we take. Our actions can lead to a positive reward or negative reward.
The set of actions we take define our policy (π) and the rewards we get in return defines our value (V). Our task here is to maximize our rewards by choosing the correct policy. So we have to maximize for all possible values of S for a time t.


 
Fig(i)
The bot using the reinforcement learning and neural network can enhance its learning ability and can finish the given task in most of the complex arcade game environment.
Reinforcement learning has a framework similar to that of supervised learning, where an input is a frame that is run through a network model and the network produces an output. But in reinforced learning, the network does not have a target label. It means that this technique does not have a dataset to work on to determine the output at a particular input frame. Reinforcement learning is achieved by training a policy network and one of the ways to train the policy network is using policy gradient.
From the Fig(i), we understand the working of reinforcement learning. The policy network is trained with the help of this rewards and penalty. For every episode of actions that has fetched the agent penalty, such episode is unlikely to happen in the future. On continuous training, such penalty resulting episodes are filtered from happening and episodes that fetch the more rewards and preferred.
II.	BACKGROUND AND RELATED WORK
Reinforcement Learning [4] motivated from the observation of human and animal behaviors that learn from interaction with the environment which has been widely used in AI. SARSA (State-Action-Reward-State-Action) [5] is an online variation of RL in which the reward is given right after the action is chosen. One of the main issues in RL for games is the high dimensionality of state and action spaces. Modular Reinforcement Learning [6] is one of the approaches to solve the massive state-action pairs’ problem in RL. It reduces the heavy search space by splitting the states into smaller tasks which have its own reinforcement learner.
Reinforcement Learning is an area of machine learning inspired by behaviorist psychology and has been studied in areas such as genetic algorithms, statistics, game theory, control theory and checkers. An RL agent learns by selecting actions that yield the greatest accumulated numerical reward over time rather than seeking an immediate reward. In machine learning, the environment is typically represented as a Markov Decision Process (MDP) and decisions made by the agent are called a policy π (a probability distribution for selecting actions at each state). The goal of the agent is to find the optimal policy π∗, a policy that maximizes accumulated rewards over time [12]. RL proceeds by trying actions in a particular state and updating an evaluation function which assigns expected reward values to the available actions based on the received, possibly delayed, reward. Dynamic Programming (DP) is the selected RL algorithm for this paper, as was used in the original implementation of the IRL problem. More recent versions may use alternative methods. See [12] for more details on the DP algorithm.
Imitating human-like behavior in action games is a challenging but intriguing task in Artificial Intelligence research, with various strategies being employed to solve the human-like imitation problem. Imitating player behavior has several key benefits: We can create games with more intelligent and believable NPCs1 , and opponents that do not react to players in a pre-determined fashion, regardless of context [6]. Game play can be dynamically altered to adapt to different players by their features of play (their playing “style” as well as their “skill”) to sustain their engagement with the game longer [7], and learned AI agents are able to help game companies test the strength of game AI and discover defects or limitations before its release to the market. In the research [7] they consider learning human-like behavior via Markov decision processes without being explicitly given a reward function and learning to perform the task by observing expert’s demonstration. Individual players often have characteristic styles when playing the game, and this method attempts to find the behaviors which make them unique. During play sessions of the game “Super Mario” they calculate player’s behavior policies and reward functions by applying inverse reinforcement learning to the player’s actions in game.
In the paper referenced [11] the convergence rate of reinforcement learning is improved, a method of training Non-Player Character (NPC) in games using Sarsa learning algorithm is proposed. The artificial neural network is used to approximate the value function. In order to make better use of experience, this paper sets up double neural networks, and uses experience memory to store experience, and uses experience replay to speed up the convergence of Sarsa learning. Using the method presented in this paper to train NPC, we can find the NPC which is trained by the method has more learning ability than the classical reinforcement learning. SARSA learning algorithm using experience replay Sarsa learning is an on-policy learning algorithm, the iterative of which for the value function is the method of strict temporal-difference learning. Sarsa learning differs from Q learning. The Sarsa learning algorithm chooses the action according to the current state t s , and uses   greedy algorithm to select action t p . After action t p is performed, the current state t s moves to next state t1 s ,and gets rewards R . When agent in state t1 s , uses the   greedy algorithm to select next action t1 p.
Apprenticeship Learning via Inverse Reinforcement Learning - When teaching a young child to play a game, it is much easier and more practical to demonstrate it to the child rather than listing all the game rules. Then the child will learn by trying to mimic the demonstrator’s performance and grasp the task gradually. Learning from an expert by watching, imitating, or from demonstration is called apprenticeship learning (AL) [9], which employs IRL to solve such a problem. Inverse Reinforcement Learning (IRL) problems work in the opposite way to reinforcement learning - when there is no explicitly given reward function, we can use an IRL algorithm to derive a reward function by observing an expert’s behavior throughout the MDP environment. Rewards are mapped to features within the states to reflect the importance of those features to the expert. This analysis of expert behavior yields a policy that attempts to perform in a manner close to the expert. 
The gameplay in Super Mario Bros consists in moving the player-controlled character, Mario, through 2-D levels, which are viewed sideways. Mario can walk and run to the right and left, jump, and (depending on which state he is in) shoot fireballs. Gravity acts on Mario, making it necessary to jump over holes to get past them. Mario can be in one of three states: small, big (can crush some objects by jumping into them from below), and fire (can shoot fireballs). Getting hurt by an enemy means changing to previous mode or dying. While the main goal is to get to the end of the level, auxiliary goals include gaining a high score by collecting items and killing enemies and clearing the level as fast as possible. The Challenges of Playing Infinite Mario Bros [10]: Several features make Super/Infinite Mario Bros particularly interesting from an AI or RL perspective. The most important of these is the potentially very rich and high-dimensional environment representation. When a human player plays the game, he views a small part of the current level from the side, with the screen centered on Mario. Still, this view often includes dozens of objects such as brick blocks, enemies, and collectable items. The action space, while discrete, is also rather large. For example, to complete a very simple Mario level (with no enemies and only small and few holes and obstacles) it might be enough to keep walking right and jump whenever there is something (hole or obstacle) immediately in front of Mario. A controller that does this should be easy to learn. To complete the same level while collecting as many as possible of the coins present on the same level likely demands some planning skills, such as smashing a power-up block to retrieve a mushroom that makes Mario big so that he can retrieve the coins hidden behind a brick block. More advanced levels, including most of those in the original Super Mario Bros game, require a varied behavior repertoire just to complete. How to complete Super Mario Bros in minimal time while collecting the highest score is still the subject of intense competition among human players.
III.	PROPOSITION
Reinforcement learning has a challenging problem in the field of arcade gaming. The policy is trained with the help of rewards and penalties, that is resulted only after an episode of actions.
   
Fig(ii)
Consider figure(ii), where in episode 1 that is a sequence of actions that leads to a reward, whereas the episode 2 leads to failure. When closely looking at all the actions in episode 2, most of the actions were correct until the last action caused the episode to fail, the policy network considers that the actions in this episode will lead failure and thus avoids taking that action. In order for the agent to decide this it takes a ton amount of time to determine the action that lead to this failure from all the actions in the episode. This type of reward setup is called sparse reward setup.
 
Fig(iii)
 Sparse reward setup also fails completely in games such as “Montezuma’s Revenge” depicted in figure iii, where it takes impossibly long time to achieve the goal of completing the game. In this game the goal is to move about jumping across the gaps climbing up and down and also escaping the skull moving in the basement. According to the sparse reward function, reward is seen only after the key is obtained. So the network will not be able to determine if the sequence of moves it is making leads to reward or not until it gets the key which takes infinitely longer time thus sparse reward function fails
An alternate to sparse reward function was Reward shaping. It is manually designing a reward function that guides your policy to some desired behavior. Such as giving a reward to the agent for every move that it makes avoids the skull in Montezuma’s revenge game. One major disadvantage is that this reward function needs to be done separately for every Atari game that is certainly not scalable. Another problem is Alignment problem, where the agent determines ways to achieve maximum rewards points alone and not achieve the goal.
Deep Q-learning is a special type of Reinforcement Learning technique where the Q-function is learnt by a deep neural network. Given the environment’s state as an image input to this network, it tries to predict the expected final reward for all possible actions like a regression problem. The action with the maximum predicted Q-value is chosen as our action to be taken in the environment. Hence the name Deep Q-Learning. In this environment, an expert is made to play an arcade game for a couple of hours. While doing so, all the frames are logged into the dataset which is later fed to a neural network.
Let’s say our environment is in a particular state s, and upon taking an action a, it changes to state s’. For this particular action, the immediate reward you observe in the environment is r. Any set of actions that follow this action will have their own immediate rewards, until you stop interacting due to a positive or a negative experience. These are called future rewards. Thus, for the current state s, we will try to estimate out of all actions possible which action will fetch us the maximum immediate + future reward, denoted by Q(s,a) called the Q-function. This gives us Q(s,a) = r + γ * Q(s’,a’)which denotes the expected final reward by taking action a in state s. Here, γ is a discount factor to account for uncertainty in predicting the future, thus we want to trust the present a bit more than the future.
 
Fig(iv)
Deep Q-learning is a special type of Reinforcement Learning technique where the Q-function is learnt by a deep neural network. Given the environment’s state as an image input to this network, it tries to predict the expected final reward for all possible actions like a regression problem.
 
Fig(v)
Record the in-game actions and decisions helped to train an end-to-end Deep Learning based bot without having to hard-code a single rule of the game. With limited training, the bot was able already picked up on basic rules of the game: making movements towards the goal and putting the ball in the back of the net. 
This agent can get very close to human level performance with many more hours of training data. There is a serious problem with this approach where, the bot that is trained, only tries to mimic the expert human. But this bot will become no better than a human which is not what is expected.
A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in 
challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The 
tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were 
trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce 
an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game 
rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also 
the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality 
move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved 
superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.
A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.
 
Fig (vi)
IV.	CONCLUDING SOLUTION
In Reinforcement learning, a sparse reward training is really hard and reward shaping is not an optimal solution. The Q-learning which is a special type of learning only tries to mimic the expert human and any better than the human players which is exactly what is required. This paper comes up with a solution that is not completely based on reinforcement learning but partially supervised reinforcement learning that uses human knowledge to build the network base and later the network learns with reinforcement learning. Consider figure(iii), a human playing that game can easily retrieve the key since he can visually recognize the obstacles such as gaps and the skull. The humans are able to do this since they have a prior knowledge about how an obstacle looks like and how all it can be avoided and also the goal that needs to be reached. This knowledge is what is lacking in an agent that needs to learn the environment by making all possible moves and altering the network which helps it to make the best possible move. In supervised learning, the agent is fed with the frames of the human player’s game play as a dataset. With this dataset the agent learns how the human plays under various environmental situations and gets trained. This way the agent will never become better than the human. But with the help of partial supervised reinforcement learning, the prior knowledge that humans have can be made use to train the agent by giving the human players action moves under various situations to avoid obstacles such as seen in Apprenticeship learning and trains its network. Then the network is trained using reinforcement learning’s reward shaping technique thus, thus making it better than humans. Impossible arcade games can now be played by the bot.